Statistical & Methodological Audit: PerryPicks NBA Prediction System (v3)
Executive summary (≤10 bullets)
Current System Overview: The PerryPicks v3 system consists of two regression models (at halftime and end of Q3) predicting final NBA game outcomes (total points and point margin)[1]. It uses a two-head architecture (one model output for total, one for margin) implemented primarily with gradient boosting trees (plus baselines like ridge regression and random forest)[2]. Models are trained on historical games with a walk-forward temporal split (train on ~500 past games, test on next 200, then advance) to mimic real-time prediction[3]. Feature inputs are team-level statistics from the portion of the game before the prediction point (e.g. first half stats for the halftime model) – including shooting efficiency metrics, estimated possessions, turnover rates, and other box score aggregates[4][5]. No explicit lagged features from prior games are used in the current version (only same-game stats up to that point).
Prediction Targets & Outputs: The system predicts two continuous targets per game: total points (sum of home and away final scores) and margin (home minus away final score)[6]. These come from separate model “heads” but use the same feature set. By predicting total and margin, one can derive implied scores for each team. However, because they are modeled separately, predictions can occasionally be incoherent (e.g. implying negative team scores if total < margin, see Target Coherence risk below). The Q3 model (using game state at end of 3rd quarter) is structurally similar to the halftime model[7] but currently trained on a much smaller sample (only 100 games, a prototype)[8].
Training Data & Methodology: Training data comes from NBA play-by-play and box score APIs, plus Vegas market data for betting lines[9]. The entire 2023-24 season (~2796 games) was used for the halftime model, providing a reasonably large sample[10], whereas the Q3 model’s sample is very limited[8]. The training regimen uses a walk-forward backtesting approach[3]: models are retrained periodically as new games arrive, always training on past games and testing on future games (no random shuffling, preserving temporal order[11]). For example, train on games 1–500, test on 501–700, then extend train to 700 and test 701–900, etc. This approach aims to yield unbiased performance estimates under realistic conditions (avoiding training on future data). No separate static hold-out set is kept; instead, performance is averaged over these rolling test windows. Hyperparameters for models (e.g. tree depth, learning rate) appear to have been set a priori or via previous version tuning[12] – there is no mention of a nested CV for hyperparameter optimization in v3 (potential risk of overfitting if any test data influenced these choices).
Feature Construction: All features are derived from information available up to the prediction point in each game. For halftime predictions, this means first-half team statistics (points, shooting percentages, turnovers, fouls, etc.), and for Q3 it includes stats up to end of 3rd quarter. The JSON lists categories like shooting efficiency (e.g. effective FG%, free-throw rate)[13], pace/possessions (estimated possessions and points per possession)[14], play-style metrics (rebound rates, turnover rates)[15], and raw counts of events (rebounds, fouls, timeouts, etc.)[5]. Each game’s feature vector likely concatenates home and away team stats or their differences (to represent the matchup). Market features: It’s noted that pre-game betting lines (spread, over/under) are available features from The Odds API[16], but using them for predictions can introduce leakage since they encapsulate informed priors. The documentation flags a feature_leakage risk if full-game market lines are used in the halftime model[17]. In practice, a drop_market_priors option was tested to exclude these lines, indicating the team is aware of the leakage concern and tries to mitigate it by only using first-half stats for halftime predictions.
Uncertainty Estimation & Calibration: The system produces prediction intervals (PIs) for the final score predictions via two methods[18][19]. Method 1: Gaussian Residuals – assume prediction errors are normally distributed; the model takes the mean prediction $\mu$ and adds/subtracts a z-score times the residual standard deviation (sigma) to form an interval[20]. For example, for ~80% confidence, $z\approx1.28$ (one-tailed 90th percentile) is used, so interval = $\mu \pm 1.2816\cdot\sigma$[21]. This method is fast but assumes symmetric, homoscedastic errors (which may not hold – if errors are skewed or vary by context, this interval can misestimate tail risks)[22]. Method 2: Quantile Regression – train separate models to directly predict the 10th percentile (lower bound) and 90th percentile (upper bound) of the outcome[23]. This is more flexible (no normality assumption; can capture asymmetric error distribution)[24], but it doubles the modeling workload and complexity (two extra models) and can be less stable with limited data. The system also includes a post-hoc calibration step: after initial training, it computes residuals on training data and estimates the empirical 10% and 90% quantiles of those residuals[25]. These quantiles (resid_q10, resid_q90) are added to the point predictions to adjust the interval bounds[26]. This essentially fine-tunes the interval to achieve an empirical ~80% coverage on training data (target coverage 0.8 is mentioned)[27]. The assumption here is that the error distribution is stationary going forward[28] – i.e. residuals in future games will follow the same distribution as in training. If that assumption breaks (e.g. model errors get larger in a new season or for certain teams), the calibrated intervals could mis-calibrate. Additionally, a joint residual covariance between total and margin is modeled (a 2x2 covariance matrix via multi-output residual modeling)[29] so that correlation between errors in total vs margin predictions is accounted for when simulating outcomes – a recognition that if one prediction is off, the other might be off in a related way.
Evaluation Metrics & Betting Outcome: The system tracks several performance metrics[30]. For point predictions: Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are used to summarize accuracy[31]. For interval quality: the primary metrics are interval coverage (the fraction of times the true outcome fell within the predicted interval) and interval width (average range length)[32]. The goal is high coverage (targeting 80%) with minimal width, which indicates precise yet reliable predictions. They also mention probability calibration metrics like Brier Score and ECE (Expected Calibration Error)[33], implying the model might output win probabilities or other probabilities (perhaps derived from predicted score distributions). Finally, if the system is used for betting, betting performance metrics are tracked: ROI (return on investment), an edge threshold (only betting when model’s implied edge is above a certain percentage), and max drawdown[34]. These ensure that any betting strategy based on the model is evaluated on financial outcomes. (Notably, the JSON doesn’t detail how these bets are chosen – presumably using model vs. sportsbook lines comparisons – so part of our audit will ensure this evaluation is done rigorously, without selection bias or overfitting to historical odds.)
Statistical Assumptions: The methodology makes several implicit statistical assumptions[35]: (1) Independence of games: Each game is treated as an independent observation. This ignores that teams appear multiple times and season-level effects (e.g. a team’s improvement or fatigue over a season) – we assess this assumption’s plausibility below. (2) Stationarity: It assumes the relationships between features and outcomes remain stable over time[36]. In other words, a model trained on past seasons is expected to perform similarly on future seasons (no major concept drift). Given rule changes, evolving play styles (e.g. the NBA’s increasing reliance on 3-point shots), and roster turnover, this is only partially plausible – short-term (within one season) it may hold, but over multiple seasons the distribution of scores and feature effects can shift. (3) Normality of residuals: The Gaussian PI method explicitly assumes prediction errors are approximately normally distributed[37] (and constant variance). NBA score prediction errors might be roughly bell-shaped but can have outliers (e.g. unexpected blowouts) – heavy tails or skewness can occur, making the normal approximation imperfect (we see this in actual coverage vs expected). (4) Linearity (for ridge baseline): One of the baseline models is a ridge regression[38], which assumes a linear relationship between features and outcome. The primary model (GBM) is nonlinear, so overall the system doesn’t strictly assume linearity; it can capture interactions and nonlinear effects with trees. (5) Homoscedasticity: The initial modeling did not account for varying error variance – except by splitting by scenario (halftime vs Q3). It implicitly treated error spread as constant (until the post calibration step adjusts quantiles globally). This may be violated: e.g. predicting a high-scoring game might inherently have more variance than a low-scoring game prediction. (6) Temporal ordering for model evaluation: The training/test split explicitly assumes no future data leaks into training[11], which is a good practice. We will verify if any feature engineering might violate this. Assessment: Game independence is not strictly true (teams have autocorrelation over games), but treating games as independent is a common simplifying assumption – it means confidence intervals on metrics might be too narrow if there’s clustering by team/season. Stationarity is questionable across seasons (NBA trends drift), but if models are retrained frequently or only within-season, it’s more valid. Normal residual assumption is iffy; we’ll propose distribution-free methods. Homoscedasticity is likely false (error variance depends on game context), so addressing that is part of our upgrades (heteroscedastic models).
Key Findings – Rigor and Risks: Our audit identified several critical issues. Data leakage is a top concern: even though the system tries to avoid using future data, the inclusion of pre-game betting lines and any unintentional use of full-game stats at halftime could give the model information that’s effectively from the future (the betting line “knows” something about expected final outcomes)[17]. Temporal features absence: The current model lacks team form and momentum features (no lagged performance or recent streak indicators)[39], relying on a stationarity assumption that is likely violated – this can hurt predictive power and fail to adjust to drift (e.g. if a team drastically improves mid-season). Small sample for Q3 model: The Q3 model’s statistics are not reliable (100 games is too few to validate any patterns)[40]; any performance reported for it has huge uncertainty. Backtest robustness: The walk-forward scheme is good, but the evaluation might still be optimistic if hyperparameters were tuned on these same backtests. We suggest a more robust rolling evaluation and nested CV for tuning to avoid any subtle lookahead. Target coherence: Because total and margin are predicted separately, about 0.5–1% of games (estimated) could have incoherent predictions (like predicting Team A to win by 10 but total only 15 points – impossible). This hasn’t been explicitly measured in v3, and we consider it a flaw in the modeling approach. Lastly, betting strategy evaluation needs to be done very carefully out-of-sample to avoid overfitting to historical odds; any strategy should be fixed before seeing test results to yield a true measure of ROI (the team should guard against trying many betting rules and cherry-picking the one that would have worked historically, a form of multiple testing bias).
Recommendations Overview: We provide a comprehensive set of recommendations. In evaluation, we urge adopting a more rigorous rolling-origin validation (evaluate on every chunk of games to get error distribution) and season-wise holdouts (test on entire unseen seasons) to ensure the model truly generalizes. We also introduce formal statistical tests (e.g. Diebold–Mariano test for comparing model predictions[41]) and confidence intervals for metrics (via block bootstrap) to distinguish real improvements from noise. For data leakage, we propose specific tests to flush out any hidden leakage and strict data cutoffs in feature engineering. In uncertainty quantification, we recommend moving to conformal prediction intervals, which make minimal assumptions and can guarantee coverage even under dependent, non-normal errors[42], and to model predictive heteroscedasticity (allowing some games to have wider/narrower intervals based on context) rather than assuming one-size-fits-all uncertainty. For feature engineering, adding temporally informed features (recent performance, rest days, injuries proxy) is a high priority, as is possibly incorporating a hierarchical model component to account for team-specific effects. We outline an experimentation roadmap with 6–12 concrete experiments to validate each proposed change (e.g. ablation tests for leakage, adding lagged features, joint modeling for coherence, new interval methods, drift adaptation strategies, etc.). Finally, we stress the importance of pre-defined decision thresholds and statistical Go/No-Go criteria for deployment: no change should go live unless it’s demonstrated with high confidence (p<0.05 and practical effect size) that it outperforms the status quo on out-of-sample data. By following these recommendations, PerryPicks can enhance its predictive accuracy, robustness to change, and the credibility of its betting advice, moving from a prototype towards a statistically sound production system.
Ranked critical flaws / risks
Data leakage or temporal contamination: Severity: Critical. The most serious risk is that the model may be inadvertently using information that wouldn’t be available at prediction time, inflating its perceived performance. For example, using the final Vegas closing lines (which are set based on pre-game information about the game) in the halftime model can act as a leak – at halftime you wouldn’t normally have a “better” total line than what was available pre-game, yet if the model relies on that line it’s effectively using wisdom that already prices in likely game outcomes[17]. Other subtle leakage paths include using rolling averages that include the current game’s data, or any feature derived from full-game stats (even something like “current point spread at half” if taken from live data feeds could leak how the game is progressing beyond the raw stats). Late-game information must not creep into earlier predictions. Assessment: The JSON indicates this risk was recognized (“drop_market_priors” flag)[17], but we consider any reliance on betting market data to be handled with extreme caution or eliminated in model evaluation to get a true measure of model skill. Data leakage is plausible given the complexity of features; if present, it will make backtest results overly optimistic and won’t translate to real deployment. This risk is very plausible in sports data where the timeline must be carefully respected and things like odds can act as shortcuts. We will design specific leakage tests to root this out (see Experiment 1 below). If leakage is confirmed, it’s a show-stopper flaw – the model’s reported accuracy cannot be trusted until fixed.
Lack of temporal features & non-stationarity handling: Severity: High. The current feature set does not include prior games’ information (no lagged performance trends, no indicators of team momentum, rest, etc.)[39]. This means the model assumes that a team’s performance today can be fully explained by their first half stats of that game (or first 3 quarters), without context of whether the team has been over- or under-performing lately. It also assumes model parameters learned from past data apply equally well at all times – i.e. stationarity[36]. In reality, NBA team performance has autocorrelation: a team on a long road trip or second night of back-to-back tends to score less (fatigue), a team missing a star player will have different output (injury effect), and league-wide scoring can trend up or down each season (e.g. the three-point revolution leading to higher totals). Without features to capture these, the model can systematically err. For instance, if league scoring increases year-over-year, a model trained on older data will underestimate totals in a new season – violating stationarity. Because no season-specific or recent-form calibration is done, the stationarity assumption is not very plausible in NBA data beyond short time spans. This flaw manifests as potentially higher error on games that are out-of-distribution (e.g. early season games with new rosters, or late-season games with tanking teams) and as drift over time – performance might degrade if the model isn’t retrained frequently. The risk is high because it directly impacts prediction accuracy and the model’s ability to stay relevant. We will address this by introducing lagged features and by implementing drift detection with retraining triggers.
Small sample size for the Q3 model (insufficient power): Severity: High. The Q3 model is noted as having only 100 games of data[10], which is dramatically low for a robust predictive model. Statistically, any evaluation metrics (MAE, RMSE) from 100 points have huge confidence intervals – the error could easily fluctuate by a large margin with a different set of 100 games. Moreover, with such a small sample, complex models (even a GBM) might overfit patterns that are just noise. For example, if in those 100 games a particular team had a lot of blowouts, the model might latch onto team identity in a way that doesn’t generalize. The JSON explicitly flags that 100 is too low and that ~2000+ games are needed for production[43]. The risk here is twofold: (a) the Q3 model’s performance estimates are unreliable (it might appear “great” or “terrible” just by luck of the sample), and (b) decisions made from the Q3 model (like betting on 4th quarter lines) could be baseless. From a statistical power perspective, detecting any improvement or difference is near impossible with N=100 – standard error of metrics will be high. This is less a flaw in methodology and more a limitation, but it is critical: no strong conclusions or deployments should be made from the Q3 model until the sample size is vastly increased. The halftime model, with ~2796 game examples[44], is in much better shape data-wise (though if that spans only one season, it’s effectively 1230 unique games – possibly they counted each team’s stats as separate entries). In any case, the smaller the sample, the more one must rely on simpler models or strong priors to avoid overfitting. The Q3 model in its current state should be treated as a prototype only (which the team acknowledges)[43]. Our audit will suggest not using it in production and prioritizing data collection.
Target incoherence (independent total vs margin predictions): Severity: Medium. The separate modeling of total and margin can yield incoherent predictions, meaning combinations of total and margin that are not mathematically possible or extremely unlikely. Since:
home_score = (total + margin) / 2,
away_score = (total – margin) / 2,
any prediction should satisfy that both home_score and away_score are non-negative integers. If the models predict total = 200 and margin = 50, that implies home_score = 125, away_score = 75 (which is fine). But if total = 180 and margin = 200, that implies away_score = -10, home_score = 190, which is impossible (negative score). Even a less extreme case, say predicted margin = 15 in favor of home, but predicted total = 140, gives implied scores 77.5 vs 62.5 – non-integer and suggests some inconsistency in modeling (in reality scores are integers, though the model works in real numbers). Incoherence could also mean the model might sometimes predict the underdog to win (margin sign flips) but the probabilities implied by margin vs total might conflict with probabilities implied by predicted scores. The JSON notes as a strength the “two-head architecture (joint modeling of total/margin)”[45], but in truth the heads are only loosely joint (they likely share some model structure but are effectively separate targets). The inclusion of a residual covariance model[29] is an attempt to account for their relationship in simulation, but doesn’t guarantee coherent point forecasts. Why this matters: In betting or downstream use, an incoherent prediction can be problematic (e.g. which do we trust, the total prediction or margin prediction if they conflict about expected scores?). It also indicates the model might not be capturing some fundamental relationships; a joint model might better learn, for instance, that extremely low totals tend to have closer margins (perhaps) or other correlations. The risk is moderate – it won’t always produce nonsense, but even occasional incoherent outputs reduce user confidence and could be exploited or need post-processing. We view it as a flaw in methodology that can be improved by moving to a multivariate modeling approach (predicting home and away points directly or adding a consistency constraint). Until addressed, there may be a need to manually adjust incoherent predictions (e.g. if total < |margin|, one could clamp margin to be within total range), but that ad-hoc fix isn’t ideal.
Evaluation procedure & overfitting risk: Severity: Medium. While the use of walk-forward validation is a strength, there are still concerns about the rigor of model evaluation and potential overfitting to backtest. It’s not stated how hyperparameters were chosen – if any test set performance was used in choosing model settings or feature sets, that introduces bias. The absence of formal statistical tests for improvements is noted as a weakness[46]. This means the team might be iterating on the model (adding features, tweaking parameters) and judging success by looking at backtest error reductions. Without a proper nested validation, this process can lead to backtest overfitting – essentially, tuning to the idiosyncrasies of the evaluation set. The risk is that reported gains (say v3 vs v2) might not actually hold on truly unseen data. Additionally, the current backtest uses relatively large test blocks (200 games) which is fine, but it may not account for season boundary effects (if those 200 happen to span partial seasons). The lack of a dedicated untouched test (like say the entire 2022-23 season as final evaluation) means we might not have a completely unbiased measure. Another risk here is multiple comparisons – if the team tried several modeling ideas and only reported the best, the likelihood of a false positive (thinking an improvement works when it was just luck on that test) is higher. In summary, the evaluation, as currently practiced, might be inadvertently optimistic. This is of medium severity: it doesn’t break the model’s current performance, but it means future changes could be mishandled or the model might not be as good as believed when truly deployed. We will mitigate this by establishing stricter evaluation protocols (rolling cross-validation, significance tests, etc.) to ensure robustness and guard against any selection bias in the modeling process.
Betting performance evaluation bias: Severity: Medium. If the system is used to inform bets, a critical risk is selection bias in assessing betting results. This occurs if one tests many betting strategies on historical data and picks the one with the best ROI – that strategy is likely overfit to the past and may not do well going forward. For example, one might test thresholds like “bet if predicted cover probability > 55%, >57%, >60%” etc., and find 57% gave the best past ROI. But unless that threshold was decided beforehand or tested on a separate set, the reported ROI is an overestimate. The JSON lists “Edge Threshold – Minimum expected value” as a metric[34], implying they consider only betting when the model’s edge is above some threshold. It’s unclear if that threshold was optimized on the same data. Similarly, looking at maximum drawdown after the fact and saying it’s acceptable doesn’t guarantee future drawdowns won’t be worse. Without pre-registration of betting criteria and an out-of-sample test, one can fool oneself into thinking the model is a money-maker when it’s not. Additionally, multiple testing – e.g. evaluating ROI on totals, spreads, moneyline picks, various bet types – increases the chance of finding at least one “profitable” looking strategy in backtest purely by luck. This is a risk to the project’s real-world viability (losing money) and to its credibility. Probability calibration plays a role here: if the model says something is a 60% win probability, it should win ~60% of the time in reality for bets based on that to break even (given odds). If the model is miscalibrated (common if it underestimates variance or covariate shift), the betting strategy will perform worse than expected. We rank this risk medium because it doesn’t affect the predictive model’s accuracy per se, but it is crucial for any practical deployment in betting. Our recommendations include stringent out-of-sample betting tests (treating one whole season as a “paper trading” experiment) and using statistical tests for ROI (to see if it’s distinguishable from zero after variance). Essentially, the model’s betting performance must be evaluated with the same rigor as its predictive performance – otherwise one might deploy a strategy that has a negative edge in reality.
Recommended new evaluation protocol (step-by-step)
To ensure unbiased performance estimates and prevent any hidden overfitting, we recommend overhauling the evaluation procedure as follows:
Rolling-origin evaluation across time: Instead of a few large backtest blocks, use a sliding window or expanding window evaluation to generate many test points. For example, starting from the earliest season available, train the model on an initial window (e.g. first 500 games) and test on the next 100 games, then slide forward 100 games (add those to training) and test on the subsequent 100, and so on. This produces multiple test sets covering the entire timeline. Each test window is chronologically later than its train window (preserving temporal ordering strictly[11]). By examining error on each test window, we can observe how performance evolves over time (e.g. does MAE increase in later seasons?), and we reduce variance in our estimates by averaging many folds. This rolling-origin backtest is a form of time-series cross-validation that yields a more robust picture than a single train/test split[3]. It also allows us to detect if the model starts to perform worse in certain periods (indicative of drift or structural changes, which a single split might miss).
Season-wise holdout validation: In addition to sliding windows, implement a leave-one-season-out testing regime[47]. For example, train on all games from 2018–2022 and test on the entire 2023 season as one block. Do this for multiple seasons (train on 2018–2019, test on 2020; train on 2018–2020, test on 2021; etc.). Season-level splits ensure that test data truly represent a “new context” the model hasn’t seen – including any offseason changes and league trends. This is a stringent test of generalization. By comparing performance season by season, we also get a sense of variability: maybe the model does very well in 2020 but worse in 2021, indicating sensitivity to that season’s style (or perhaps injuries/COVID that year). A season-aware evaluation helps ensure the model isn’t overfit to quirks of a particular season. It directly addresses stationarity – if a model trained on previous seasons consistently underperforms on the next season, that flags non-stationarity (model needs updating or features for that shift). This protocol also avoids mixing seasons in training/test, which is important if seasons are somewhat independent (each season might have a different baseline pace or scoring level).
Nested cross-validation for model tuning: Implement a nested CV or rolling validation for hyperparameter tuning to avoid optimistic bias. Specifically, whenever we want to tune hyperparameters or feature selections, do not use the final test sets for this. Instead, within each training window (from step 1 or 2), perform an inner validation: e.g. further split the training window’s data into an inner train and inner validation (the last 10-20% of the training window can serve as a validation set) to choose the best hyperparameters. For example, if using an expanding window, at fold “train on 2018–2021, test 2022,” within 2018–2021 we could treat 2021 as validation and 2018–2020 to train different hyperparameter candidates. Pick the best model by validation performance, then retrain on full 2018–2021 with those params and evaluate on 2022. This way, test data remains truly unseen until final evaluation. This nested approach should be automated for each fold to avoid human bias (don’t peek at test 2022 to pick hyperparameters). Yes, it’s computationally heavier, but given the data sizes (a few thousand games) it’s feasible. By doing this, we eliminate hyperparameter leakage – where one might inadvertently choose a model that happened to do well on test due to luck. The JSON’s mention of “Nested cross-validation (hyperparameter tuning)” as a potential improvement[48] aligns with this step. Overall, the model selection criteria will be based on these inner validations averaged over time, rather than just overall test error, to ensure generalizable settings.
Statistical significance and confidence intervals for metrics: After implementing the above, we will not just report single-number metrics, but also quantify the uncertainty in those metrics. Use a block bootstrap on the sequence of prediction errors to compute confidence intervals for MAE, RMSE, etc.[49]. In practice, because game outcomes within a season may be correlated (same team’s games, etc.), we employ a moving block bootstrap: sample blocks of consecutive games (to preserve any streaks or autocorrelation) and compute metrics on those resampled series[49]. Repeat 1000+ times to get a distribution for, say, “season 2023 MAE”. This gives a margin of error for MAE (e.g. “MAE = 7.5 ± 0.3 with 95% confidence”). Similarly, when comparing two models, use these resampled differences to see if zero difference lies outside the CI. We also explicitly perform Diebold–Mariano tests when comparing forecast accuracy across models[41]. For example, if we introduce a new feature, we’ll compute the loss (say squared error or absolute error) for each game for both the baseline and new model, then apply the DM test to see if the mean difference is statistically distinguishable from zero (accounting for any autocorrelation in errors). A significant p-value (<0.05) would indicate the new model truly outperforms the old on that sample of games. We will treat improvements as real only if they pass such tests and show consistent direction of improvement across multiple folds or seasons (not just a single-season fluke). By enforcing effect size + significance thresholds (e.g. “New model must reduce RMSE by at least 5% and p<0.05”), we add rigor to model iteration. This addresses the earlier risk of overfitting: any change must prove itself on unseen data in a statistically sound way.
Detailed error analysis and edge-case validation: The evaluation protocol should include analysis beyond aggregate metrics. After each backtest, perform residual analysis: check if errors are unbiased (mean ~ 0), and plot residuals over time to see if there are periods where the model consistently underestimates or overestimates (which could indicate unmodeled effects or drift). We also propose evaluating conditional performance – for instance, split the games into high-total vs low-total (maybe top 25% vs bottom 25% of actual totals) and compute MAE in each. Or split by spread (games with a heavy favorite vs pick’em games) to see if margin errors are larger in some regime. This helps identify if, say, the model struggles on very high-scoring games (perhaps due to cap on training data) or on blowouts vs close games. If any such pattern is found, it feeds back into feature engineering (for example, if low-scoring games are harder, maybe a feature about defense or pace is missing). Another facet: calibration analysis on evaluation – take the predicted probabilities (if model gives a win probability or cover probability) and bucket them (e.g. games predicted ~60% win chance for home) and see how often home actually won in those games. Compute the Expected Calibration Error (ECE)[50] for classification outcomes like win/lose or over/under. This ensures our probabilities make sense and would be crucial if deploying to betting (a well-calibrated model won’t systematically overestimate edges). All these analyses should be included in evaluation reports, not just “MAE=7”. Essentially, treat the evaluation as a full diagnostic of the model’s performance characteristics.
Pre-registered deployment tests (A/B testing or paper trading): Before fully deploying any model (especially one driving bets or significant decisions), conduct a controlled out-of-sample test in a forward-looking manner. For example, paper trade a season: Take the 2024-25 season, use the model (frozen at whatever version) to predict each day’s games in real-time (only past data up to that day in training). Log all decisions (predictions, bets, etc.) but do not actually act on them financially until season’s end. At season end, evaluate how the model did in this truly out-of-sample, forward-in-time test. This is effectively using the future as a hidden test set. It’s the ultimate evaluation because no matter how clever our backtests are, there’s always a chance of hidden biases – the live forward test is the gold standard. If the model meets performance targets in this live simulation (e.g. it achieved the expected 80% interval coverage, maintained ROI above threshold, etc.), then we can deploy with more confidence. If not, we iterate further. For less critical deployments, one could also do an A/B test in production – e.g. use the new model for half the games (randomly or stratified by some criteria) and old model for the other half, to compare outcomes in parallel. But in an NBA prediction context, it might be simpler to just shadow-test the new model vs historical outcomes. The key is that we define go/no-go criteria in advance (for instance: “if the new model’s RMSE is not worse than current by more than 1 point and it provides >2% better coverage, we go with it; if ROI is negative, we do not use for betting”). Pre-registering such criteria guards against us rationalizing poor results after the fact (“oh, it underperformed, but maybe it was just a weird season…”). In summary, the evaluation protocol moves from just retrospective analysis to a proactive, statistically grounded validation process ensuring that only demonstrably better models are deployed.
Uncertainty overhaul (default + fallback + scoring)
Improving prediction intervals is a priority, as the current methods (basic normal approximation and quantile regression) have limitations. We recommend two modern approaches for uncertainty quantification, and outline an evaluation scheme for interval quality:
Adopt Conformal Prediction as the default PI method: Conformal prediction is a distribution-free technique that can wrap around any model to produce prediction intervals with a guaranteed coverage (assuming errors are exchangeable)[51]. For our time-series context, we suggest a sliding-window or online conformal approach that accounts for temporal dependencies. The idea: maintain a buffer of recent prediction residuals as a calibration set, and for each new game prediction, determine the interval such that, say, 80% of past residuals fall within that error magnitude[42]. For example, if the halftime model’s last 300 games had an absolute residuals median of 7 and 80th percentile of 12, then an 80% conformal PI for the next game would be [predicted_score – 12, predicted_score + 12]. As new games occur, slide the window: drop the oldest residual, incorporate the newest. This method naturally adapts to any change in error distribution – if the model’s errors get larger due to, say, a higher scoring era, the intervals widen. Several conformal variants exist for time series: one approach is Ensemble Batch Conformal (EnbPI) which uses ensemble models and past residuals with a moving window[52]. Another is an adaptive conformal method where we assign more weight to recent residuals (to handle gradual drift)[53]. Implementation without leakage is critical: we must only use residuals from games prior to the game we’re predicting. This means in a live setting we would predict game i’s interval using residuals from games < i. In backtests, we mimic that by always splitting calibration sets chronologically. We should also ensure exchangeability or independence assumptions are approximately met by using a large enough window (so one outlier doesn’t swing the quantile too much) and perhaps stratifying by context if needed (though a simpler approach is usually fine). Advantages: Conformal PIs do not assume normality or symmetry – they take the actual empirical error distribution, so if our model tends to under-predict extremes, that will be reflected. They also are valid under minimal assumptions; even if residuals are not i.i.d., methods like EnbPI provide approximate coverage guarantees under mixing conditions[54][55]. Evaluation: We will check marginal coverage – e.g. over a season, 80% of true finals should fall in 80% PIs. We’ll also check conditional coverage – e.g. coverage for games where the model was very confident (maybe a narrow interval) vs games with wide interval. Conformal prediction can be assessed via conditional coverage plots or size-stratified coverage (ensuring no strong pattern like “when interval is narrow, coverage drops”) – if we see such a pattern, we might adjust our strategy (maybe use a slightly larger quantile or a hybrid with parametric). Sharpness (interval width) might be slightly larger than the old Gaussian method because we’re being more conservative about tails, but it will be more trustworthy. In summary, conformal prediction becomes our primary interval generation method due to its robustness and ease of implementation (just needing past residuals storage and sorting). We expect it to correct the likely underestimation of uncertainty from the simple Gaussian method (which assumed homoscedastic normal errors). Notably, conformal can be applied separately for total and margin predictions or even jointly (though joint conformal is more complex; initially, do separately such that each marginal target gets proper coverage).
Use an Ensemble Bootstrap method as a fallback interval approach: As a secondary method (for validation and potentially deployment alongside), we recommend bootstrap ensembles to estimate prediction distributions. This involves training an ensemble of models (or using the existing GBM with different seeds or subsamples) and generating predictions from each, then taking quantiles across the ensemble predictions. For instance, train 30 gradient boosting models on bootstrap-resampled training sets; to predict a game, get 30 predicted totals and margins, sort them – the 10th percentile of those predictions is the lower bound, 90th percentile is upper bound. This approach captures both model uncertainty (if data is limited, different models give different outcomes) and inherent noise. It does not rely on asymptotic normality. Barber et al. (2023) show that such an approach combined with a moving residual window (like EnbPI) can give theoretical coverage guarantees under certain mixing assumptions[52][56]. In practice: We can implement a simplified version: train an ensemble once on the whole training set (with different random seeds or bagging), and for each new game, take the distribution of the ensemble’s outputs. If concept drift is a concern, we could retrain or re-weight the ensemble periodically. This is computationally heavier than a single model, but with modern compute and the relatively small data (a few thousand games), it’s manageable offline. The advantage of having this fallback is twofold: (a) It provides a check on the conformal method – if both methods agree on interval coverage and width, we’re confident. If they diverge (say bootstrap ensemble gives much wider intervals), it might indicate model instability or that conformal’s window missed some bad cases. (b) In a non-stationary scenario, an approach like EnbPI actually combines ensemble and moving residuals: we might implement that exactly – train an ensemble and use a leave-one-out residuals approach as described in the literature[56]. For simplicity, the bootstrap percentile method itself is a valid way to get prediction intervals (though not guaranteed coverage unless calibrated). We will likely calibrate it: e.g. if the empirical coverage of the ensemble’s 80% interval was only 70% in backtest, we can adjust (perhaps take a broader percentile like 95% range to achieve 80% actual coverage). Use case: The ensemble approach might be especially useful for the Q3 model once it has more data – with small data, parametric assumptions fail, so bootstrap can help quantify uncertainty. Also, if we move to neural networks in future, Monte Carlo dropout or Bayesian ensembling would play a similar role. In any case, the fallback here means if, for some reason, conformal is hard to deploy live (maybe due to needing to store residuals or update quantiles on the fly), we have the ensemble quantile method as a simpler (but computational) backup. It’s also educational to present both to stakeholders: “Conformal says interval ~ [A, B], Ensemble says [C, D]” – if they largely agree, confidence in the interval is high.
Incorporate heteroscedastic predictive modeling: In addition to post-processing methods for intervals, we suggest the model itself be extended to predict its own uncertainty as a function of features. This can be done by switching to or augmenting with a distributional regression model. One strong option is NGBoost (Natural Gradient Boosting) which is designed for probabilistic prediction by outputting a parametric distribution for the target[57][58]. For example, NGBoost can output a Normal($\mu_i$, $\sigma_i^2$) for each game’s total points, where both $\mu_i$ and $\sigma_i$ are functions of the input features. It trains by maximizing the likelihood of actual outcomes under these predicted distributions (or minimizing a scoring rule like CRPS). By doing so, it learns to predict larger $\sigma$ for games that are inherently harder to predict (e.g. maybe high-tempo teams yield more variance) and smaller $\sigma$ for more stable games. Another approach is to keep our GBM and add a second head that predicts variance (this would involve customizing the loss to a likelihood-based one – some gradient boosting libraries allow specifying a distribution like Gaussian with mean and variance to fit). We could also train a separate model for absolute error or variance given the features (a two-stage approach: first predict outcome, then predict squared error). Regardless of approach, the goal is a heteroscedastic model: error variance is no longer assumed constant; the model will output an interval that can widen or narrow depending on the matchup and game state. For instance, if at halftime the score is tied 50-50, our model might predict total ~200 with a larger uncertainty (because games tied can swing more) versus if it’s 80-50 at half, it might predict total ~210 with smaller uncertainty (blowouts might have more predictable pacing in second half). We will validate this by looking at, say, PIT (Probability Integral Transform) histograms of the model’s probabilistic predictions[59]. If the model says 80% probability total is under X, that should be true 80% of the time if well-calibrated. We’d also check residuals vs. predicted variance: plot the model’s predicted $\sigma_i$ against the actual absolute error $|y_i - \hat{y}_i|$. We expect to see that on average, when predicted $\sigma$ is high, the actual errors are larger, and when $\sigma$ is low, errors are smaller – ideally points should align around y = x in a residual-vs-uncertainty plot normalized by distribution (for a perfect model, standardized residuals $r_i = (y_i-\hat{y}_i)/\hat{\sigma}_i$ should follow $N(0,1)$). We’ll use calibration curves for uncertainty: e.g. take all predictions where model predicted 90% interval = [L_i, U_i] and check what fraction of those had actual outcomes within [L_i, U_i] (this is similar to checking coverage conditional on the interval width or predicted confidence). If we find miscalibration (common initially), we might apply a secondary calibration (like scaling the predicted variances) based on validation data. NGBoost in particular has been shown to produce good calibrated uncertainties while maintaining accuracy, especially on smaller datasets[60]. It’s a relatively straightforward drop-in replacement for our GBM, with the benefit that we get an explicit probability distribution for each prediction, enabling direct probability queries (like “what’s the probability the total > 230?” can be answered). Even if we adopt conformal as default for intervals, having a heteroscedastic model is complementary: it can improve the conformal method by providing a better starting point (we might use varying-size residual windows or condition on features in conformal if needed), and it directly addresses the homoscedasticity flaw. In summary, we will likely pilot NGBoost or similar as an experiment; if it yields better uncertainty estimates (as measured by interval sharpness for a given coverage, or by log-likelihood scores), we may use its outputs either as the final product or as an input to conformal (conformal can be applied on top of any method, including NGBoost, to guarantee coverage).
Interval evaluation metrics (coverage, width, interval score): To compare and monitor the performance of uncertainty quantification methods, we will set up an evaluation table and process that includes:
Coverage at various levels: Calculate the empirical coverage of prediction intervals at different nominal confidence levels: 50%, 60%, 70%, 80%, 90%, 95%. For example, for 80%, ideally ~80% of actual outcomes fall within the 80% PI. We’ll do this for each method (Gaussian, Quantile, Conformal, NGBoost, etc.) side by side. This immediately shows if a method is over-confident (coverage lower than nominal) or too conservative (coverage higher than nominal but possibly with excessive width).
Average interval width: Compute the mean width of the 80% (or any level of interest) interval for each method. A narrower interval for the same coverage is better (sharper predictions). If one method achieves 80% coverage with intervals 10 points wide on average and another needs 15 points, that’s a big difference in precision. We will be mindful that an extremely narrow interval that fails coverage is not useful – hence we consider width only in context of coverage. We can also report a scaled width (e.g. width as percentage of total points to normalize across different scoring levels).
Interval Score (Winkler Score): Use a proper scoring rule for interval forecasts to aggregate performance. The Winkler score for an $\alpha$ confidence interval (equivalently $(1-\alpha)$ coverage) rewards narrow intervals but penalizes lack of coverage[61]. Essentially, if $L_i, U_i$ is the predicted interval for game $i$ and $y_i$ the actual outcome, the interval score is $(U_i - L_i) + \frac{2}{\alpha} \cdot (L_i - y_i)+ + \frac{2}{\alpha} \cdot (y_i - U_i)+$, where $(x)_+$ is the positive part. This means we pay a penalty if $y_i$ is below $L_i$ or above $U_i$ (missed interval), proportional to how far out it lies, on top of the width cost[62]. A lower interval score is better. We will compute the mean Winkler score for each method on the test set. This one number balances coverage and width (and is a proper score, meaning it’s minimized by the true distribution).
Conditional coverage and width analysis: As mentioned, we’ll break down interval performance by scenarios. For example, consider close games vs blowouts: perhaps define “close game” as actual margin <5 points, and see if our 90% interval coverage is lower for that subset (maybe our model underestimates volatility in close games which can swing wildly). Or by total points bins: low-scoring games might have relatively higher variance percentage-wise. Also by time: early season vs late season coverage. Another important conditional metric is to check coverage vs interval size: group predictions by predicted interval width (e.g. deciles of predicted width) – ideally, in each group, the fraction of actuals inside interval $\approx$ the nominal confidence. If we see, say, when the model predicts a very tight interval (group1), the empirical coverage is much lower than nominal, that indicates overconfidence in those cases (maybe the model is too sure about some games). We can address that by maybe inflating variance for that segment or investigating what those games are.
Example evaluation table snippet: (to be included in reports)
| Method | 80% Cov | 80% Avg Width | 90% Cov | 90% Avg Width | Winkler score (80%) | Winkler score (90%) |
|-------------------|--------:|--------------:|--------:|--------------:|--------------------:|--------------------:|
| Gaussian (resid) | 72% | 9.5 | 85% | 14.2 | 8.3 | 12.1 |
| Quantile Reg | 78% | 10.1 | 88% | 15.0 | 8.5 | 12.4 |
| Conformal (window)| 81% | 11.0 | 91% | 15.5 | 9.0 | 13.0 |
| NGBoost (param) | 79% | 9.8 | 89% | 14.5 | 8.2 | 12.0 |
(Illustrative numbers)
This would show, for instance, that the initial Gaussian method was under-covering (72% vs nominal 80%), quantile did better, conformal achieved slightly above 80% (perhaps a tad conservative), NGBoost got close to nominal with smaller width, etc. The interval score helps decide overall – lower is better.
Calibration plots: Additionally, for probabilistic predictions (like if we have a full distribution from NGBoost or quantile), we can compute the Continuous Ranked Probability Score (CRPS)[63] as a metric and do probability calibration curves (PIT histogram as mentioned). But those are more diagnostic. For communicating interval performance, coverage and width are most interpretable.
All these metrics will be calculated on genuine test sets (per the new evaluation protocol) so we trust them. We will use them not just to pick the best method, but to continuously monitor the production system – e.g. every month, re-check that 80% of actuals fell in the 80% PI. If we see a drift (say coverage dropping to 70%), that’s a red flag to recalibrate or update the model. In summary, this evaluation approach ensures our uncertainty quantification is both well-calibrated and as sharp as possible – a cornerstone for high-stakes use like betting or risk management.
Modeling & feature roadmap (Must / Next / Nice)
We categorize the improvements into Must-have (immediate fixes and additions), Next (medium-term enhancements), and Nice-to-have (long-term exploratory ideas). This roadmap prioritizes fixes that address the biggest risks and those that yield the greatest accuracy/robustness gains for the effort.
Must-have (Short-term, crucial):
Leakage prevention and data integrity: Immediately audit all features to eliminate any possibility of using information not available at prediction time. This means ensuring the halftime model uses only first-half stats (and possibly pre-game info that is truly pre-game, like season averages, but not anything that implicitly contains second-half info). We will disable use of betting lines in model training unless specifically evaluating a market-informed model. (The JSON’s note of using drop_market_priors=true in backtests[17] should become the default configuration for a purer evaluation of model skill.) Additionally, implement strict time indexing in data pipelines: for any computed feature, include a check that its timestamp < game halftime timestamp for the halftime model, etc. For example, if constructing a feature “team’s last 5 games average score”, the code should explicitly filter games with date < current_game_date. We also intend to run placebo tests (like shuffling certain feature timelines) to double-check no information is leaking (see Experiments below). This is a must because any leakage, even subtle, invalidates all other results. By sealing off leakage, we might see a drop in backtest performance (if indeed the model was unknowingly “cheating”), but that’s a reality check we need.
Incorporate essential temporal features: Begin integrating lagged performance indicators as features. The JSON already suggests many in “potential_improvements”[64] – we will prioritize those with highest likely impact:
Recent game averages: e.g. team’s average points scored and allowed in the last 5 games, last 10 games. Also the average point margin in last N games (to capture team strength/momentum). These provide context if a team’s offense or defense is trending up or down.
Rest and fatigue: days since the team’s last game; a boolean if the team is on second night of a back-to-back; perhaps a count of games in last 7 days. Fatigue can significantly affect scoring (teams score a few points less on back-to-back).
Travel and home/away context: a feature for whether the game is home or away (if not already implicit in team designation) and maybe if the team traveled across time zones (some datasets include distance traveled). A simpler proxy: a flag if this is the last game of a road trip or if the team is returning home after a road trip. Such schedule context can subtly impact performance.
Head-to-head matchup history: how these two teams fared against each other in recent matchups (e.g. average total and margin in their last 3 meetings). This might capture stylistic matchups (some teams, when facing each other, produce slower games, etc.)[65]. However, careful: this can overfit if not enough data (teams change), but it’s worth testing.
Injury or lineup changes (proxy): If we can’t get real-time injury data, we might use a proxy like difference between expected lineup quality and actual (perhaps from betting lines movement). A crude approach: compare the opening betting line to the closing line – a big shift might imply a star injury news. Or track each team’s top players’ average points last 5 games; a sudden drop could indicate missing players. This is harder to quantify, so might come slightly later, but we flag it as important.
“Hotness” features: like win streak length or covering-the-spread streak. A team on a winning streak might have confidence/momentum (though this can also regress, but it’s a feature to test).
Implementation: All these features must be created without future leak – we’ll likely build a rolling features dataset where for each game we pre-compute these from prior games. Because it’s a must-have, we’d aim to include at least the recent averages and rest days in the very next version of the model. These additions directly address the stationarity and context issues identified. We expect them to improve predictive accuracy (teams that are performing well above season average in recent games likely continue that in the next game, etc.) and help the model adjust to short-term fluctuations that a static model misses.
Enhance model training with more data (especially for Q3): Increasing sample size is a must, particularly for the Q3 model. We should gather historical data beyond 2023-24 for halftime model (if not already done) – ideally several seasons (last 5-10 seasons) to give the model more variety and to better detect season-over-season differences (which informs stationarity tests). For the Q3 model, since it’s new, aggressively collect as many games as possible: if API rate limits are an issue[66], implement caching and use available historical game data (maybe try alternative sources or scrape archival data) to expand this to at least a few hundred games. In the meantime, treat any insights from Q3 model with caution. Another angle: generate synthetic data for Q3 model validation – for example, use the halftime model to simulate what it would predict at Q3 and final, to augment Q3 data. However, that could just reinforce halftime patterns, so better is to simply gather real data. The model compliance note says Q3 needs 2000+ for production[43], so that’s our target. In summary, “more data” might not sound like a model change, but it’s fundamental – we must not be complacent with a tiny dataset. This is marked must-have because without adequate data, other fancy improvements won’t matter (noise will dominate).
Joint modeling for coherence: Implement a solution to ensure predicted total and margin are coherent. The most straightforward is to predict home and away scores separately (as a pair). For example, use a multi-output regression tree model or train two models but with a coupling: one model predicts Home_final_score, another predicts Away_final_score. They can share features; we might even include the predictions of one in the other (iteratively). Since total = home+away and margin = home-away, this automatically yields consistent totals and margins. There’s a nuance: ensuring non-negativity and integer predictions is not typical in regression, but since scores are relatively large, a small negative prediction can be handled by clamping to 0 (if it ever happens). Alternatively, one can model total and margin jointly by a bivariate distribution approach (e.g. bivariate normal of errors). However, a simpler deterministic approach should work: for instance, predict total as before, and predict margin as before, but then adjust: if an incoherent combination arises, we could adjust margin down to fit within total. But adjusting ad-hoc could introduce bias. A better approach: add a constraint during training that total >= |margin| always. Some gradient boosting libraries don’t support such constraints directly, so the two-model approach might be easier. Another option: use a copula to join the distributions of total and margin at the prediction stage – e.g. predict distribution of total and conditional distribution of margin given total. This might be too complex for now. So our plan in short term: train a unified model that outputs two targets (many frameworks allow multi-output trees). If not, we will train separate models for home and away final scores using identical features; then derive predictions for total & margin from those. This should inherently prevent impossible values and might yield better accuracy since each model can specialize (one learns offense of home, one away). We consider this must-have because incoherent outputs, while maybe rare, undermine confidence and indicate suboptimal modeling. We’ll carefully test this change against the current separate approach – it’s possible separate models slightly minimize their own loss better, but we value the consistency and combined learning as more important for a robust system.
Robust regularization and simplicity where needed: As we add features (especially lagged ones) and possibly more complex models, we must guard against overfitting. It’s a “must” to apply techniques like cross-validated early stopping for boosting, use regularization (the existing GBM hyperparams already include things like min_samples_leaf=30, subsample=0.8[67] which are good). We might increase regularization given more features. If we incorporate team-specific effects, consider partial pooling (like a ridge on team indicators). The idea is to ensure the model remains generalizable. This is more of a practice than a feature, but it’s crucial when we expand feature space drastically.
Next (Mid-term, high-impact improvements):
Advanced model architectures and ensembles: Once the basics are in place, we move to testing stronger modeling approaches:
Gradient Boosting improvements: Try alternative GBM implementations like LightGBM or CatBoost. CatBoost could naturally handle categorical features like team IDs or opponent IDs through its encoding, which might improve performance if we include such features (team ID as categorical might allow the model to learn team-specific offsets). Also CatBoost handles ordinal features, but here mostly everything is numeric. LightGBM might train faster on large data and allow easy cross-validation. We should compare these with our current implementation (which might be XGBoost or sklearn’s HistGBM as implied by JSON). Hyperparameter tuning with modern tools (optuna or grid search under nested CV) can also squeeze extra performance.
Stacking and blending: The JSON suggests ensemble averaging and stacking[68]. We can employ a simple ensemble of models: e.g. average predictions from ridge, random forest, and GBM, which may improve robustness (since they have different biases). More ambitiously, a stacked model: e.g. use the predictions of ridge and GBM as inputs to a meta-model that learns to correct biases. However, stacking in time-series must be done carefully to avoid leaking future info in the meta-training (meta-model should be trained on out-of-fold predictions from past data only). If done right, stacking could yield a small boost (if one model picks up linear trends and another nonlinear interactions, meta can combine).
Hierarchical/Bayesian models: Incorporate hierarchical pooling by team or other grouping. For example, use a mixed-effects model where each team has a random intercept for points scored and conceded. In a Bayesian sense, this could shrink team-specific biases. Practically, we could compute features like “team offensive rating” and “team defensive rating” learned from data (like team-specific constants) and update those each season. Or use a partial pooling regression (there are Bayesian regression packages or even lme4 in R which could be used offline to estimate team effects). These team effects can then be fed into the GBM as additional features. This acknowledges that not all teams are equal – the model currently might implicitly learn team differences via averages of first-half performance, but explicitly giving a prior for each team might stabilize predictions especially when a team has an outlier game. This is a next-step because it’s a bit complex and needs careful evaluation (danger: overfitting if done naively, but Bayesian shrinkage helps).
Alternative distributions: We might experiment with models tailored to count data since points are non-negative counts. E.g. a Poisson regression for each team’s score (common in soccer modeling) or a Negative Binomial if over-dispersed. NBA scores might be high enough to approximate normal, but a Poisson might capture that variability in a more theoretically sound way. We could compare a straightforward Poisson GLM (with log link using features) as a baseline or part of ensemble. This touches on feature interpretability as well – such models might make it clearer how possessions or shooting efficiency translate to expected points.
Evaluate via paired testing: For each candidate new model or ensemble, we’d use the same backtest splits to compare against the baseline (ensuring fairness). For example, if we test CatBoost vs current GBM, we run both on identical train/test folds and compute metrics. Only if it consistently outperforms do we consider replacing. This systematic approach ensures we don’t get fooled by variance.
Refined hyperparameter tuning (nested CV automation): By now, we should set up an automatic hyperparameter search process that is aware of the time-series structure. For instance, use a Bayesian optimization or grid search that for each candidate hyperparam set, evaluates via a mini walk-forward on the training data. Specifically, use something like 3-fold time-split CV on the training set to get a score, and have the optimizer propose new params. The parameters to tune include: tree depth, learning rate, number of trees, min samples per leaf, regularization penalties, etc. We also might tune the choice of which features or lags to include (feature selection could be part of this process). We will impose a tuning budget – e.g. at most 50 parameter sets evaluated – to avoid overfitting via over-tuning. Stopping rules: if after X iterations the improvement is < y, stop. This ensures we don’t endlessly tweak and accidentally fit quirks of the tuning data. Once the best params are found, we lock them and do a final evaluation on the outer test. This nested approach was discussed earlier and is a mid-term goal once the pipeline is robust. It might require significant compute but yields confidence that we are at a good spot in model space. Additionally, as new data comes (new seasons), we might schedule periodic re-tuning (perhaps each off-season re-tune hyperparams on last few seasons). We must be careful to avoid leakage in tuning: never tune on the actual future test target. For example, if we are tuning a model that will be used in 2024 season, we should tune using only data up to 2023.
Improved probability calibration: If the model outputs probabilities (e.g. probability Team A wins, or probability total > line), ensure these are calibrated. One mid-term improvement is to use Platt scaling or isotonic regression on validation data to adjust the probabilities[50]. For example, if our model’s raw output says “home win probability = 0.65”, but historically when it says 0.65, the home team won only 60% of the time, we can fit a calibration curve to map 0.65 -> 0.60. This is crucial for betting: an edge calculation (probability > implied probability from odds) is sensitive to calibration. We’ll collect probability predictions and outcomes from training/val to learn a calibration mapping. This can be done separately for different bet types (spread vs total). ECE (Expected Calibration Error) will be monitored and we’ll aim to reduce it via such post-processing. In essence, the model’s confidence should reflect true frequency – this can be achieved by these monotonic recalibration techniques. This is a mid-term step once the underlying predictive performance is acceptable, as calibration mainly fixes the probabilistic interpretation.
Automated drift detection & adaptation: As a next-level robustness feature, implement a monitoring system for model drift. Using the metrics outlined in evaluation, we can on an ongoing basis compute things like: distribution shift in inputs (e.g. average points per game creeping up) and output residuals shift. Specifically, compute Population Stability Index (PSI) periodically for key features like league average pace, or teams’ stats[69][70]. A high PSI (above a threshold, e.g. >0.25) indicates the feature distribution has changed significantly from training baseline. Also track the model’s error over a rolling window: if we see a significant upward CUSUM in residuals[71] or a significant drop in calibration (e.g. actual coverage of 80% PI over last 50 games is only 60%), that signals drift. With such detection, plan an adaptive response: either retrain the model on more recent data (rolling window retraining) or at least recalibrate the predictions. The JSON suggests retraining might be needed when environment changes (rules, etc.). We propose a simple trigger: if performance metrics on a moving window degrade beyond a set threshold (e.g. MAE is 20% worse than validation MAE for 10 games in a row), flag for retraining. Retraining could mean updating the model weights with new data or fully re-fitting from scratch on an expanding dataset including the new data (depending on the algorithm’s capabilities). This is a mid-term because first we need to know normal performance variability (hence after deploying, gather some data to set these thresholds). But designing the framework now is good. Another part of adaptation is to consider dynamic features – e.g. continuously update team strength ratings as the season progresses (like an Elo rating that adjusts each game). Those can be input to the model to help it quickly adjust to new team abilities. That blurs into model vs data updates, but is part of adaptation.
Data enrichment: As a next step, integrate additional data sources if possible. For example, if player-level data becomes accessible in real-time, incorporate it: e.g. a feature for “star player X is out” or use the Vegas line movement as a feature (if it jumps by a large amount close to game, that usually indicates breaking news). Another data aspect: incorporate live in-game metrics for the Q3 model – perhaps at Q3, one could also input how the game has deviated from expectation (like difference between current total and pre-game total line, which tells if it’s faster or slower than expected). This can refine the Q3 prediction. These enhancements might wait until we have the core working, but are on the roadmap.
Nice-to-have (Long-term, exploratory):
Deep learning approaches for sequence modeling: Investigate models that can capture temporal dependencies within games or across games in a more data-driven way. For instance, use an RNN/LSTM or even a Transformer model that takes as input the time series of scoring throughout the game (or the play-by-play sequence) and outputs a distribution for final score. Such a model could in theory understand momentum shifts or the impact of runs better than aggregate features. Similarly, a Transformer-based time-series model could take a series of past game results for both teams and current in-game stats to predict final outcome. These approaches require more data and careful training (to avoid overfitting sequences), but could potentially outperform manual feature engineering if there are subtle patterns (like how teams respond after halftime adjustments, etc.)[72]. This is labeled “nice” because it’s high effort and uncertain payoff; our current problem size (few thousand games) might be borderline for heavy deep learning, but as data grows or if we include play-by-play events (which are plentiful per game), it becomes feasible. We’d likely experiment with this offline and compare to the structured model. It might also unlock Monte Carlo simulations where the model can simulate play-by-play of the 4th quarter to produce an uncertainty distribution.
Full-game simulators or hybrid physics models: Another long-term idea is to simulate the remainder of the game using a model of possessions. For example, given halftime stats, simulate play-by-play of second half by sampling from estimated team offense/defense efficiencies. This could produce a distribution of outcomes (like a bespoke simulator rather than a generic ML model). This would incorporate more basketball-specific knowledge (pace, etc.). It could also account for things like if a game is a blowout at Q3, the pace often slows, bench players come in (garbage time effect). A simulator could adjust for that, whereas a static model might not. This requires detailed modeling and is more of a bespoke solution – nice to explore if the ML approach plateaus.
Integration of real-time updates for live betting: Pushing the system to make predictions not just at fixed points (halftime, Q3) but continuously (e.g. at any time during Q4) would be a nice capability. It would involve feeding the latest game state (maybe every minute’s score) into a model that updates win probabilities or expected totals. We’d need very optimized code and data feeds, plus retraining models to handle partial quarter data. This is beyond current scope but could be a differentiator in the market.
User-facing explainability and insights: Adding features for explainability is valuable long-term. For example, after each prediction, output the top 3 factors that influenced it (using SHAP values or permutation importance)[73]. E.g. “Predicted total is high mainly because both teams have a fast pace (+8 points) and they scored a lot in first half (+5), offset slightly by poor 3-point shooting so far (-2).” Such explanations build trust with users or help analysts understand model behavior. Achieving this requires calculating feature importance for each prediction, which SHAP can do for tree models. It’s computationally heavy but can be done offline or on-demand. Also performing global importance analysis helps in feature engineering feedback loop (we see which features matter most and confirm they make sense or if any spurious ones are overly influential). This is nice-to-have as it doesn’t directly improve accuracy, but it improves adoption and debugging.
Continuous improvement process: Put in place a mechanism to experiment regularly with new features or techniques (perhaps using automated machine learning pipelines). For example, every off-season, rerun an AutoML on the last few seasons to see if any new interactions or model types emerge that beat the current champion model. This isn’t a feature per se, but a practice – ensuring the model stays state-of-the-art. Given the fast evolving ML landscape, we consider it prudent to allocate some time for exploratory trials (like trying a new library, or testing a Gaussian Process model for uncertainty as mentioned in JSON improvements[59]) and benchmarking them.
The above roadmap ensures that we first fix critical issues (leakage, missing temporal context, sample size), then enhance the model’s predictive power and reliability (advanced models, tuning, drift handling), and finally explore cutting-edge approaches to maintain an edge. Each step will be empirically validated via the rigorous evaluation protocol, so we only move to the next phase once the current one yields measurable benefits.
Concrete experiment plan (6–12 experiments)
We propose a series of controlled experiments to validate and quantify the improvements suggested. Each experiment has a clear hypothesis, implementation change, evaluation split, success metrics, statistical test, and a go/no-go decision rule. These experiments should be conducted in sequence (or in parallel if independent), with findings informing whether to adopt the change.
Experiment 1: Data leakage detection via feature delay/placebo
Hypothesis: If a feature is leaking future information, then breaking or misaligning that feature’s timing will significantly worsen model performance. Conversely, if there’s no leakage, the model’s accuracy should remain the same when that feature is removed or time-shifted. We suspect that features like pre-game betting lines might be acting as a pseudo-leak (since they embed crowd wisdom about the final outcome).
Change: Identify potential leakage-prone features (e.g. the Vegas total line, spread, or any “full-game” stats inadvertently used). Create a modified dataset where these features are either dropped or replaced with a placebo version. For example, for each game’s Vegas total line, assign it the Vegas total line of a different random game (or the previous day’s line) – effectively breaking any real informative link. Train the model on this placebo feature dataset. Additionally, try shifting any cumulatively computed features: e.g. if there was a feature “cumulative season average up to game”, shift it one game ahead so it doesn’t include the current game.
Split: Use a representative subset of data (e.g. one full season) for a focused test. Split it temporally: train on first 70% of games, test on last 30% (or use the rolling backtest procedure). The key is to compare two models on the same test set: one with the original features, one with the “leakage-disabled” features.
Metrics: Compare the MAE and RMSE of predictions between the original model and the no-leak model. Also specifically monitor if any bias is introduced (e.g. does one model consistently predict higher totals?). If a feature like the Vegas line was heavily used, the original model might have lower error but perhaps only because it was “cheating” – we need to see if removing it causes a statistically significant degradation. We will also track feature importance: in the original model, how important was the suspect feature? If it was top-ranked and performance drops without it, it’s a red flag for leakage (or at least overreliance).
Statistical test: Use a paired Diebold–Mariano test[41] on the error series of the two models (original vs modified) over the test games. Null hypothesis: no difference in predictive accuracy. If p < 0.05 and the modified (no-leak) model’s errors are larger, it suggests the original had an advantage likely due to that feature. Also perform a permutation test: randomly shuffle the model assignment to error pairs and recompute difference many times to see if the observed difference is beyond random chance (this is similar to DM but can be done non-parametrically).
Go/No-Go rule: Go (no leakage) if the performance of the no-leak model is not significantly worse (e.g. MAE difference < 0.5 and p-value > 0.1), meaning the feature wasn’t giving unfair info or the model can cope without it. No-Go (leakage found) if removing the feature causes a substantial and statistically significant drop in performance (e.g. MAE increases by >5% and test shows p < 0.01), AND we determine that the only reason for that could be information not available at prediction time. In that case, the feature (or modeling approach) must be eliminated or revised before any further model improvements are trusted. (If it’s the Vegas line feature, likely we drop it from the production model and accept a slightly higher error that is honest.) This experiment essentially safeguards all subsequent ones by ensuring we’re not building on a leaked foundation.
Experiment 2: Adding lagged features & recent performance
Hypothesis: Incorporating recent games’ performance as features will improve predictive accuracy. Teams’ last N-game averages and rest days will provide additional signal beyond same-game stats, thus reducing MAE/RMSE. We expect, for example, that including a team’s average score in the past 5 games and whether they are on back-to-back will meaningfully increase model predictive power (especially for totals), as it captures momentum and fatigue that are not in the first-half box score.
Change: Engineer a set of new features: for each game, for each team – (a) average points scored in last 5 games, (b) average points allowed in last 5 games, (c) win/loss streak length entering the game, (d) days of rest before the game, (e) perhaps a simple Elo or power rating derived from past games. Integrate these into the halftime model’s training data (ensuring they are computed using only games prior to the current game). Then retrain the model with these features added (keeping all other settings the same for comparability).
Split: Use a rolling-origin evaluation across multiple seasons: e.g. train on 2018-2019 (with new features), test on 2020; then train 2018-2020, test on 2021, etc. Also perform within-season rolling test for one season (train first half of season, test second half) to see improvement in second half predictions – this is where recent-form features should shine, because by later in season, some teams fundamentally change (tanking or surging). The baseline for comparison is the same evaluation with the model without those new features. So effectively, we will run the backtest with old features and with new features.
Metrics: Primary metrics: MAE and RMSE on total points and margin predictions. We expect these to decrease with the new features. Also check if the error variance decreased (i.e. not just mean error, but are predictions more consistent?). We might see bigger improvements for certain subsets: e.g. on back-to-back games the model with rest info should do much better than the one without. So we’ll specifically measure MAE for games where a team had 0 days rest vs >1 day rest, to quantify improvement in that subset.
Statistical test: Use a paired t-test or Wilcoxon test on game-level absolute errors between the two models (for each test fold). Also aggregate results: e.g. if we have fold-wise MAEs, do a paired test across folds. Additionally, check Diebold–Mariano on the squared errors (which accounts for any autocorrelation) to confirm any improvement is not due to chance[41]. If the p-value is small (<0.05) and the new model has lower error in most folds, that’s strong evidence the features help. If results are mixed (some folds better, some not), we may analyze why (maybe certain seasons the feature helps, others not).
Go/No-Go rule: Set a threshold for minimum improvement: e.g. Go if MAE is reduced by at least ~2 points (which is a sizable jump for NBA totals) or ~2-3% relative, with statistical significance (p<0.05). Also, go if we observe improved prediction particularly for games that previously were problematic (like model is now more accurate on back-to-backs, etc.), even if overall improvement is modest – because that addresses a known weakness. No-Go if improvement is negligible (<1% or within noise) or if the added complexity doesn’t yield clear gains. In that case, we might prune some features (maybe not all lagged features are useful) or gather more data – but given domain knowledge, we expect a positive result here. If it’s a go, these new features become part of the production model.
Experiment 3: Joint modeling of home/away scores (coherence fix)
Hypothesis: Predicting home and away points with a unified model will eliminate logical inconsistencies and may improve accuracy by leveraging outcome correlation. We expect to see 0 cases of total < |margin| (impossible) in the joint model outputs, whereas the separate model might have a few. We also anticipate a slight improvement in error metrics, since the joint model can share information (for instance, if it knows total well, it can implicitly help margin prediction). At worst, accuracy might stay similar, but coherence will improve (which itself is a win).
Change: Implement a multi-output regression for final scores. Concretely, we might use an algorithm that natively supports multi-target (sklearn’s HistGradientBoostingRegressor can output multiple targets, or we train two models with a coupled strategy). For a fair test, we do the simplest: train one model to predict [home_final_score, away_final_score] simultaneously from halftime features. Compare this to two separate models (one for total, one for margin as before). Ensure the multi-output model has same complexity (we might give it the same total number of trees, etc., or let it figure out both tasks with shared trees). If multi-output is not straightforward, an alternative is to impose a post-processing: e.g. take the separate models’ outputs and adjust them to the nearest coherent combination (but that might degrade accuracy, so better to truly model jointly).
Split: Use a recent season or two for evaluation. Because coherence issues might be rare, we want a decent sample to see differences. For example, train both approaches on 2018–2022 data, test on 2023 season. Or do a rolling evaluation for multiple seasons with each approach. For each test game, record the predictions from both methods.
Metrics:
Coherence metrics: Count the number of games where predicted total < |predicted margin|. That count should be zero for the joint model by design (if it’s not zero, then something’s very wrong with the implementation). For separate models, count how often that happens (hopefully it’s small but non-zero). Also measure how far off the separate model was in those cases (could measure an “incoherence magnitude” = |margin| - total when margin exceeds total).
Accuracy metrics: MAE/RMSE for total and margin as usual. We’ll see if the joint model’s errors are on par or better. It might improve margin prediction more (since margin is a derived quantity, joint modeling might stabilize it). Also check correlation of errors: presumably, separate models might have had some redundant errors (if both overestimate scoring, margin error could cancel or double count); joint modeling might manage error correlation better. We can compute the correlation between total error and margin error in both approaches – ideally, joint modeling yields a realistic correlation structure.
Calibration: If possible, compare distributional predictions. But likely we focus on point estimates here.
Statistical test: Use a paired test on MAEs of the two approaches across games. Since each game yields a total and margin prediction, we can examine margin prediction errors specifically – are they lower variance for the joint model? Use DM test on margin errors and on total errors separately. Additionally, test if the coherence difference is significant: we could treat an incoherent prediction as a “failure” event and use McNemar’s test (paired binary test) to see if joint model has significantly fewer failures. With near zero failures for joint and some for separate, this will likely show improvement (though if separate only had say 1% incoherent, a McNemar test might be on the edge of significance due to low counts – but practically, zero is better than non-zero).
Go/No-Go rule: If the joint model achieves no incoherent outputs and maintains equal or better MAE compared to separate, it’s a GO to adopt it. We’d accept even a very small accuracy improvement or even parity, because coherence is a non-negotiable logical requirement for a production-quality system. The only reason for no-go would be if, for some reason, the joint model had notably worse accuracy (say MAE increased by >1 point or p<0.05 showing old separate model was better). That might happen if the multi-output model over-constrains learning. In such case, we might try a hybrid: e.g. predict total first, then margin conditional on total. But assuming our multi-output boosting handles it, we likely go with it. This experiment thus likely leads to adopting the joint modeling approach, ensuring consistency (which is crucial for user trust and for any use like generating probability distributions of team scores).
Experiment 4: Conformal vs existing prediction intervals
Hypothesis: Conformal prediction intervals will achieve more reliable coverage than the current methods. Specifically, for an 80% nominal interval, conformal will be ~80% in actual coverage, whereas the Gaussian method might be lower (under-covering in tails) and quantile regression might misestimate coverage if the model’s quantile loss wasn’t perfect. We also expect conformal to adjust to any non-stationarity – e.g. if a particular season had more variance, conformal intervals would automatically widen, something a static method wouldn’t. The trade-off might be slightly wider average intervals, but overall interval score should improve due to better coverage.
Change: Implement the sliding-window conformal interval generation on top of the point predictions of the halftime model. Use, say, the past 500 games’ residuals to form intervals for the next game (this window size can be tuned). Also implement the two baseline interval methods for comparison: (a) Gaussian 80% PI using residual std from training set, (b) Quantile regression 10th–90th percentile models. Apply all three methods to the same set of test predictions. We won’t change the point prediction model here, just the interval construction.
Split: Use a full season (or multiple seasons) as test in chronological order. Ideally, simulate the real-time process: train model on seasons up to 2022, then for 2023 season, at each game (or each day), use the model to predict and also form an interval. For conformal, calibrate on a rolling basis within 2023 (e.g. always use last N residuals from earlier in 2023 – although to start the season we might use previous season residuals as a warm-up). For Gaussian, use a fixed sigma from training, for quantile, get the model’s predicted quantiles. Collect all game outcomes and whether they hit the interval or not.
Metrics: Calculate empirical coverage of each method’s 80% intervals over the season. Also calculate 90% if we generate those. We expect: Conformal ~80%, Gaussian likely <80% if residuals were not normal or if variance increased, Quantile might be close if the quantile models generalized well, but they might be off if distribution shifted. Also compute average interval width for each. Possibly Gaussian will have narrowest (if it underestimates tails), conformal might be widest (to cover unexpected outcomes), quantile in between. Compute the interval score (Winkler) for 80% for each method to see overall performance[61]. Additionally, check conditional coverage: e.g. split games by whether the total was high or low, or by part of season, to see if any method systematically fails in some subset. Conformal’s strength is in distribution-free guarantee, but since time series breaks exchangeability a bit, it’s possible early season vs late season might differ – we’d examine that to refine the method (maybe using weighted conformal if needed).
Statistical test: Use a two-proportion z-test or Chi-square test to compare coverage rates: for example, did conformal cover significantly more often than Gaussian? If conformal covered 82% and Gaussian 75% out of, say, 1000 games, the difference is 7% with standard error ~1.3%, which is huge (z ~5.4, p ≈ 0). So that would be significant. Similarly test conformal vs quantile. For interval width, use a paired t-test on interval widths per game between methods (though widths aren’t independent of outcome, we can still just compare typical width). For interval score, do a paired comparison as well – since each game gives a score for each method, we can use DM test on those scores (treating interval score as a “loss”). Essentially, test if the mean difference in interval score is zero. If conformal has a lower (better) average interval score with p<0.05, that’s strong evidence in its favor.
Go/No-Go rule: Go with conformal as default if it achieves the target coverage (within a few percentage points of 80%) while the others do not, OR if it has the best (lowest) interval score. Even if conformal intervals are a bit wider, we value honest coverage – as long as they are not excessively wide. We might set a criterion like: must cover ~80% and average width no more than 1.2x the current method’s width. If conformal turned out to give extremely wide intervals (maybe due to small sample calibration in early season), we might refine it or consider alternatives. But assuming positive results, conformal becomes the production method for PIs. No-Go would be if, say, quantile regression already had good coverage and tighter width, making conformal redundant – but that’s unlikely across all conditions. Another no-go scenario: if conformal’s time-dependent nature proved tricky (maybe needing manual intervention to update residuals) – but that’s more implementation. Overall, we expect to adopt conformal for its robustness, using quantile regression as a secondary check. We will document coverage over time, so stakeholders see that “8 out of 10 times, the true score fell in our predicted range” which builds trust.
Experiment 5: Heteroscedastic NGBoost model evaluation
Hypothesis: A probabilistic model that predicts variance (NGBoost) will provide better calibrated uncertainty and possibly improve point accuracy slightly. We anticipate that NGBoost’s predictive intervals (derived from its predicted Normal distribution per game) will be closer to nominal coverage without needing post-calibration, and its log-likelihood score on test data will be better than that of the homoscedastic model. Point estimates (means) might improve as well because NGBoost optimizes a likelihood (which penalizes large errors more flexibly than MSE if variance is high for those points).
Change: Train an NGBoost model for the halftime scenario. Choose Normal distribution output. This yields for each game a mean prediction and a standard deviation. We will train it on the same training set as a baseline GBM was, using similar features. Note: We should ensure a fair comparison, so if possible, use the same features and no conformal calibration in this test – NGBoost vs our original GBM (with maybe Gaussian assumption). After training NGBoost, use it to predict on the test set and output its mean and 80% interval (mean ± 1.28*predicted std). Compare those to the original method’s outputs and to conformal if applicable.
Split: Use a rolling backtest or season holdout as before (e.g. train on 2018-2022, test on 2023 for both models). We might also do a cross-validation of NGBoost vs GBM on a few folds to get distribution of metrics.
Metrics:
Point prediction: MAE and RMSE of NGBoost’s mean vs GBM’s predictions. Possibly they’ll be very close; sometimes probabilistic training sacrifices a tiny bit of RMSE to get variance right – we’ll see.
Probability metrics: Compute the Negative Log Likelihood (NLL) of the test outcomes under each model’s predicted distribution. For GBM with constant variance, we can take its residual sigma and use that for NLL, or use quantile predictions to approximate a distribution. NGBoost explicitly gives NLL. A lower NLL means better probabilistic fit. Also compute Continuous Ranked Probability Score (CRPS) for each model’s distribution[63] – NGBoost is essentially minimizing a form of CRPS in training; we expect it to win here.
Calibration: For NGBoost, do a PIT histogram: take $(y_i - \mu_i)/\sigma_i$ for all test games, and see if it’s standard normal (we’d like a roughly uniform distribution of CDF values). Also check the coverage of NGBoost’s internal 80% intervals. Ideally it should be close to 80% if calibration is good. Compare that to the 80% coverage of the old method’s intervals. Also, plot predicted $\sigma_i$ vs actual absolute error $|y_i - \mu_i|$ for NGBoost – we expect a positive correlation (if NGBoost is working, large predicted sigma should correspond to larger errors on average).
Interval sharpness: Compare average interval width of NGBoost’s 80% intervals vs conformal’s or quantile’s. Sometimes NGBoost might produce narrower intervals for some games and wider for others – e.g. blowouts might get wide intervals (if it learned those are unpredictable). That’s fine as long as coverage is right. We might see an overall width similar or a bit less than conformal if NGBoost accurately knows which games need width.
Edge cases: see if NGBoost handles extremes better: e.g. does it ever predict a ridiculously high sigma or negative sigma (shouldn’t, by design sigma >0)? Are those predictions plausible?
Statistical test: Use a Diebold–Mariano test on log-likelihood of the two models. Each game provides a log-likelihood under NGBoost (log PDF of actual outcome given $\mu,\sigma$) and under baseline (assuming some fixed $\sigma$). DM test on their difference will tell if one distribution is significantly better fit[41]. Also do a paired t-test on CRPS values per game (lower CRPS is better). And test the coverage difference as before (chi-square for whether coverage hits are different).
Go/No-Go rule: This is somewhat exploratory. Go if NGBoost shows clear improvement in probabilistic metrics (significantly better NLL/CRPS) and maintains point prediction accuracy (no significant worsening of MAE). Also, if NGBoost’s calibration is markedly better (like PIT is flat, coverage on target) compared to the base method which might have miscalibration, that’s a win – especially for use cases requiring probability estimates (like computing odds of over/under). If NGBoost underperforms on MAE by a bit but is much better probabilistically, we might still adopt it for uncertainty, possibly hybridizing (e.g. use NGBoost variance with a GBM mean if needed). No-Go if it complicates things without much benefit – e.g. if MAE gets worse and calibration wasn’t improved enough to matter, we might stick to simpler conformal approach. But given literature[60], NGBoost should do well on smaller data uncertainty. We won’t deploy NGBoost immediately unless proven; this experiment gives us the info. If it’s a go, we might use NGBoost as a backend for generating intervals or at least to cross-check our conformal intervals (e.g. ensure NGBoost’s predicted 95th percentile is in line with our conformal upper bound). Possibly a combined approach could be considered (like use NGBoost mean and conformal interval for best of both worlds).
Experiment 6: Retraining cadence and concept drift simulation
Hypothesis: More frequent model retraining in presence of drift will maintain accuracy, whereas a static model will deteriorate. We think that if there’s a structural change (say a new rule that increases scoring), a model not updated will start under-predicting totals, but a model retrained after the change will adapt. Detecting that change via PSI or CUSUM and retraining at the right time yields better performance post-change. We also hypothesize that drift detection can catch the change not too much later than it happens, avoiding a long period of poor predictions.
Change: Simulate a drift in data. Option 1: Take real historical data where we suspect a drift (the NBA introduced a shorter shot clock on offensive rebound in 2018; or the bubble in 2020 had weird effects; or just split by decade). Option 2: artificially modify the data: e.g. for games after a certain date, add +10 points to every actual total (as if a rule made scoring jump). Use our base model trained on pre-drift data and see its errors on post-drift. Implement a drift detection method on the predictions: e.g. calculate a running average of residuals and use a CUSUM test[71] to raise alarm when mean error significantly deviates. Also use PSI on a feature like average points per game comparing a baseline period vs a recent period[70]. We then simulate a retraining: when drift alarm triggers at game N, we pretend we deploy a new model trained including data up to game N (or weighted recent data). Compare that to if we hadn’t (the old model continuing).
Split: This is more of a time-series experiment than a train/test split. We create a timeline: train initial model on games 1…M (pre-change), then from M+1 onward (post-change) we get predictions from: (a) static model, (b) dynamic model that is allowed to update once drift is detected (for fairness, maybe it updates only once at detection point). If using real data, we can do: train on 2016-2018 data, test through 2019-2020 where, say, 2020 had higher scoring or the bubble. If using artificial drift, easier to analyze exact effect.
Metrics: Measure MAE/RMSE before drift and after drift for static vs retrained model. We expect pre-drift both are same (since retrained model is basically same as static until drift point), and post-drift the retrained model quickly achieves lower error. We’ll measure how many games the static model stays off-spec: e.g. static model MAE might jump from 10 to 15 after drift, whereas the new model after retrain might bring it back to 10. Also measure the detection delay: how many games of bad performance occurred before the drift detection signaled? And were there false alarms before actual drift? We might adjust sensitivity to see trade-offs. We will also track the PSI values over time: for instance, PSI of score distribution each month vs training baseline – see if it crosses a threshold at drift point.
Statistical test: This is a bit different – we can treat each game post-drift as a paired observation (error_static, error_retrained). Use a paired test to see if error_retrained is significantly lower. If our artificial drift is large, it will be obvious. A t-test on that difference will confirm (likely p << 0.01 if improvement is real). We can also test if detection method triggers significantly often when no drift (to estimate false alarm rate) – e.g. run the procedure on a period with no real drift and count alarms.
Go/No-Go rule: If results show that drift detection + retraining yields substantial benefit (error reduction) in a drift scenario, then we go with implementing a monitoring system in production. This means we will, moving forward, keep an eye on distribution shifts and have a process to update the model (perhaps retrain on a rolling window or expanding window) whenever needed. The criteria from the experiment (like threshold values for PSI or CUSUM that worked well) will inform the real triggers. If by chance drift detection was too slow or didn’t help much (no-go), we might instead decide on a fixed schedule retrain (e.g. retrain model every month regardless, which is simpler but not adaptive). However, given the logic, an adaptive approach should help in any large shift. So likely go – the experiment primarily informs how to do it (what thresholds, how much data to retrain on etc.). No-go would be if detection was unreliable (maybe too many false alarms leading to unnecessary retrains that could overfit noise). If so, we’d refine the approach or set very conservative triggers.
Experiment 7: Baseline model benchmarking
Hypothesis: Our advanced model outperforms simpler baseline models by a significant margin. We want to verify that a basic model (like linear regression or using betting lines alone) is notably worse, confirming that our features and ML add value. If a baseline is close in performance, that indicates diminishing returns or possible overfitting by complex model. We suspect the ML model will be better, especially on margin prediction, but the Vegas-informed baseline might be surprisingly strong for totals (since betting markets are quite efficient).
Change: Define and implement two baselines:
(a) Historical mean baseline: Predict final total = league average total (or perhaps league avg + half-time score difference). Predict margin = 0 (basically naive, or maybe home-court average margin). This sets a very low bar.
(b) Vegas line baseline: For each game, use the closing betting lines: predicted total = closing over/under line, predicted margin = closing point spread (home minus away). This baseline uses external info (so it’s not a fair “model only” comparison, but it’s relevant because it tells how well one could do using market wisdom alone). The model currently might not use these due to leakage concerns, but for evaluation, we consider them as a predictor. We’ll ensure to only use lines that would be available at prediction time (closing line is technically right before game, so fine).
(c) Optionally, a simple ridge regression or Elastic Net using only a few intuitive features (like first-half score, maybe team averages) to see if a linear model does nearly as well. However, since we have our ridge from earlier versions, we might skip or just note it.
Train these baseline models (where applicable) on training data (the Vegas baseline doesn’t need training, it’s just direct picks). Evaluate on test sets.
Split: Use the same evaluation splits as the main model (e.g. season 2023 as test). The key is to get predictions from baseline and our model on the same games. Particularly, for Vegas baseline, collect the line info for test games. If the main model is not using that info, it’s interesting to see how close it gets to the line.
Metrics: Compare MAE and RMSE of: our model vs historical mean vs Vegas lines. The historical mean baseline might have huge error (because it ignores game-specific info). The Vegas baseline likely has decent error (bookmakers are good). If our model is good, it might beat the Vegas line slightly on average error (since the line isn’t trying to minimize MSE, it’s trying to balance bets, but generally it’s a strong predictor). Also consider correlation between our model’s predictions and Vegas lines: a high correlation would indicate our model is capturing similar factors (maybe ~0.9 correlation). But if correlation is 1, our model might just be replicating the line (which could happen if we let line slip in as feature). Ideally, our model has some independent signal. We’ll also measure calibration for margin direction: does our model or Vegas better predict winners against spread? Perhaps beyond scope, but interesting.
Statistical test: Use Diebold–Mariano test on forecast errors comparing our model vs each baseline[41]. E.g. test if our MAE is significantly lower than Vegas’s. If p<0.05 and error difference is positive, we outperform. Also do a simple sign test: count games where our prediction error < Vegas’s prediction error, see if >50% significantly (Binomial test). This paired comparison is important because if our model is not consistently better than Vegas in at least >55% games, that means it’s mostly echoing the consensus.
Go/No-Go rule: This experiment doesn’t dictate a go/no-go for a feature, but rather assesses the model’s value-add. If shockingly we find that a baseline (especially Vegas baseline) is as good or better than our model, that’s a red flag: No-Go for claiming our model is production-ready on its own. It might indicate we should actually incorporate that baseline (maybe use Vegas line as an input or model the residual from Vegas line). Conversely, if our model beats the baselines convincingly, Go to have confidence in its unique predictive power. For example, if our model MAE on totals is 9 and Vegas line MAE is 10.5, that’s a huge improvement (given Vegas is tough to beat). We might not expect that much – even matching Vegas’s accuracy would be notable. This experiment helps calibrate expectations: if our model is only as good as Vegas, one might prefer to just use Vegas for simplicity (though our model can still be used for betting if it finds slight biases). In summary, the result will influence how we communicate model performance and whether we consider adding baseline info into model. If the model underperforms a simple baseline, we definitely need to re-examine complexity (maybe it overfit and a simpler model or more data is needed).
Experiment 8: Betting strategy out-of-sample test
Hypothesis: The model’s calibrated predictions can yield a profitable betting strategy under real-world conditions, or at least identify value bets better than random. However, given the efficiency of markets, we expect modest returns and need to account for variance. The hypothesis to test is that using model-derived probabilities (or expected scores) to bet selectively will outperform a no-edge baseline (which is break-even minus commission). We also test risk metrics like drawdown to ensure viability.
Change: Define a concrete betting strategy before looking at results. For example: “Bet on the over for game totals if model’s predicted total exceeds the sportsbook total by >2 points (accounting for ~2 points of model uncertainty), bet the under if model predicts >2 points below the line. Flat bet 1 unit per qualifying bet. Do similarly for point spreads: bet on home team if model margin > spread by 1.5+ points, away if vice versa.” Also, possibly incorporate implied win probability: e.g. “if model gives team win probability > 55% but moneyline odds imply 50%, bet moneyline.” We set these thresholds based on training data or intuition (small edges, because lines are tight). Important: We then freeze these rules. Next, apply this strategy to an out-of-sample dataset (e.g. the entire 2022-23 season, which the model didn’t use for training, or even better the upcoming season in a live demo if time permits). For each game, get the model’s prediction and compare to actual Vegas lines (historical odds). Log whether a bet was made and if it won or lost (using closing lines for scoring).
Split: This is effectively using one season (or a chunk of data not seen by model) as the evaluation. No training here except the model is already trained. It’s one long test sequence for betting outcomes. We ensure that any parameters of strategy (like +2 point threshold) were not optimized on this test data – they should come from prior reasoning or a separate subset. If we had multiple seasons, we could try different strategies in each as a pseudo cross-val for strategy, but ideally we treat one season as the final exam.
Metrics:
ROI (return on investment): total profit / total amount bet. For example, +5 units profit on 100 units bet = +5% ROI. We’ll calculate this overall and by bet type (spreads vs totals vs moneylines if applicable).
Win rate vs breakeven: For spread/total bets at ~-110 odds, breakeven win rate is ~52.4%. We measure model’s win percentage on those bets. If it’s >52.4% over many bets, that’s profitable. We’ll give a confidence interval on that win rate.
Number of bets: How many games triggered a bet? Too few means strategy was very selective (maybe good for high confidence but low volume).
Max drawdown: The worst cumulative loss streak. This is important for risk assessment – e.g. even a winning strategy might have a -10 unit downswing. We’ll record the largest peak-to-valley drop in the cumulative profit curve.
Sharpe ratio or profit volatility: Standard deviation of profit (in units) per bet or per season to gauge risk vs reward.
Calibration of probabilities: We’ll also check that when the model said it has a big edge, those bets actually did well at the predicted frequency. For example, group bets by model’s estimated cover probability minus implied probability. See if higher edge group had higher win rate than lower edge group – a well-calibrated and useful model should show monotonic relationship.
Possibly subset performance: e.g. did overs vs unders perform differently (some models systematically lean one way)? Did performance degrade in second half of season (maybe model not updated)?
Statistical test: It's tricky to get significance in betting because of high variance, but we can do a bootstrap of bets to get a confidence interval for ROI or win rate. Null hypothesis ROI=0 (no edge). See if 0 is outside the CI. Also a t-test on win rate vs 0.524 (for spreads) to see if significantly above. If many bets (N large), even a 54% win rate might be significant. Also use Kelly criterion or similar to estimate if the edge is real (if model consistently identifies edges, small but positive, the distribution of outcomes should shift). We can simulate many seasons via bootstrap to see the distribution of outcomes if true win probability is model’s prediction vs if it were 50%.
Go/No-Go rule: This experiment informs whether the model is ready for actual betting deployment. Go (to using model for betting or recommending bets) if the out-of-sample ROI is positive and the results are at least trending significant (doesn’t have to be p<0.05 with one season, but e.g. win rate > 55% with decent number of bets would be convincing). Also go if drawdowns are within acceptable range given stake sizing (we can define e.g. max tolerable drawdown of 20 units – if our test had 50, that’s concerning). No-Go if the model barely breaks even or loses money in test. That would indicate either the model is not as predictive as thought (at least not against the spread/total where lines are tough) or the strategy thresholds were off. A no-go would mean we shouldn’t deploy a betting product claiming positive ROI until improvements are made or more data is gathered. We might adjust strategy (maybe be more selective or incorporate more features like injury news into betting decisions). But importantly, if no-go, it might not mean the predictive model is bad – it could be just that markets are extremely efficient. In that case, the model’s value might be more in quantifying uncertainty or providing entertainment predictions rather than beating bookmakers. However, since part of our goal is presumably to have an edge, we’d iterate on this. Regardless of outcome, this experiment should be pre-registered (we decide the strategy in advance, as we have, to avoid bias) and treated as the final arbiter of “Does our model’s edge hold up when truly tested on new data?”.
(We listed 8 experiments above. If needed, we could include additional ones, e.g., testing feature ablation (which we did partly in leakage), or experiments specifically around model explainability or multi-season generalization. But these cover the major questions in tasks A-F.)
Implementation notes (pseudo-code + pitfalls)
When implementing the above changes and experiments, there are some practical considerations and potential pitfalls to be mindful of. We outline some pseudo-code snippets and warnings:
Data preparation with proper temporal cutoff: Ensure that for each game, we only use information available up to that point. In code, this means sorting data by date and iterating. For example, building lag features:
games = sorted(all_games, key=lambda g: g.date)
rolling_stats = {team: [] for team in teams}
features = []
for game in games:
    team1, team2 = game.home_team, game.away_team
    # Compute features for team1 based on their last 5 games:
    last5_team1 = rolling_stats[team1][-5:]
    avg_points_for_t1 = np.mean([g.home_score if g.home_team==team1 else g.away_score 
                                 for g in last5_team1]) if last5_team1 else np.nan
    avg_points_against_t1 = ...  # similar for points allowed
    rest_days_t1 = (game.date - rolling_stats[team1][-1].date).days if rolling_stats[team1] else np.nan
    # do same for team2...
    feat = {
        'home_avg_pts_last5': avg_points_for_t1,
        'home_avg_allow_last5': avg_points_against_t1,
        'home_rest_days': rest_days_t1,
        # ... (and away_ counterparts)
        'halftime_score_home': game.halftime_home_points,
        'halftime_score_away': game.halftime_away_points,
        # plus existing features like efg, etc., which are computed from first-half stats of this game
    }
    features.append(feat)
    # After computing features, append this game to rolling stats for both teams for future games
    rolling_stats[team1].append(game)
    rolling_stats[team2].append(game)
Pitfall: Off-by-one errors – e.g., including the current game in the rolling window. We must append the game to history after computing features for it. Also, handling season boundaries (maybe clear rolling_stats at new season if we don’t want to carry over, or include season in keys). Ensure no future games slip in. Another pitfall is missing values for early season (a team’s first few games have no last5) – we should decide to either skip those features or fill with some default (league average) and have a flag for “no history” perhaps.
Pseudo-code for nested cross-validation (time series):
def time_series_cv_evaluate(params):
    # params is a dict of hyperparameters for model
    errors = []
    for train_start, train_end, val_end in cv_splits:  # e.g. (2018 start, 2020 end, 2021 end) etc.
        model = Model(**params)
        train_data = data[(data.date >= train_start) & (data.date <= train_end)]
        val_data = data[(data.date > train_end) & (data.date <= val_end)]
        model.fit(train_data.features, train_data.target)
        preds = model.predict(val_data.features)
        errors.append(MAE(val_data.target, preds))
    return np.mean(errors)

best_params = None
best_score = float('inf')
for params in hyperparam_grid:  # or use Bayesian opt calling time_series_cv_evaluate
    score = time_series_cv_evaluate(params)
    if score < best_score:
        best_score = score
        best_params = params
# best_params now chosen without peeking at test
model_final = Model(**best_params)
model_final.fit(full_train.features, full_train.target)
preds_test = model_final.predict(test.features)
Pitfall: This can be slow if grid is large. We might parallelize or reduce param ranges. Also, ensure that cv_splits are truly sequential and non-overlapping to simulate future prediction. Another pitfall is if data has different distributions in different seasons, an average of CV may not reflect performance on a specific season (e.g. model might overfit older seasons). It’s important to incorporate multiple years in CV to cover variation.
Updating model for drift (pseudo-code for detection):
```python model = train_initial_model(training_data_up_to_t0) residuals = deque(maxlen=100) # store last 100 residuals alert_triggered = False
for new_game in stream_of_games: # simulate streaming as season progresses pred = model.predict(new_game.features) error = new_game.actual - pred residuals.append(error)
# Check drift via residual mean CUSUM
  if len(residuals) == 100:
      resid_mean = np.mean(residuals)
      if resid_mean > drift_threshold:
          alert_triggered = True
          print(f"Drift alert at game {new_game.id}: mean residual {resid_mean:.2f} > {drift_threshold}")
  # Check drift via PSI for key feature (e.g., total points)
  recent_dist = distribution([g.total_points for g in residuals_games])  # last N games actual total
  baseline_dist = distribution(training_data.total_points)
  psi = calc_PSI(baseline_dist, recent_dist)
  if psi > psi_threshold:
      alert_triggered = True
      print(f"PSI alert at game {new_game.id}: PSI {psi:.2f} > {psi_threshold}")

  if alert_triggered:
      # Retrain model on a window or expanding set ending at this game
      retrain_start = new_game.id - retrain_window if use_sliding_window else initial_training_start
      retrain_data = data[(data.id >= retrain_start) & (data.id <= new_game.id)]
      model = train_model(retrain_data)
      alert_triggered = False  # reset alert after retraining
  ```
Pitfalls: Determining appropriate drift_threshold and psi_threshold – these might need to be learned from historical variations or set conservatively to avoid false alarms. Also, if using expanding window, the model might become slow to train as data grows, but NBA yearly data isn’t huge, so okay. We need to be careful not to retrain too frequently (overreaction) – maybe require two consecutive alerts or a significant margin over threshold. Also ensure that when we retrain, we do not include any data beyond the current point (which we don’t). Another subtlety: after retrain, residual distribution resets, so we should probably also reset the deque of residuals or the baseline for drift because the new model’s errors may have different mean (initially zeroed out). Essentially, drift detection needs a reset after model update.
Integration of conformal prediction in code:
# Assuming residuals_calib is a deque of last N residuals (absolute)
alpha = 0.2  # for 80% interval
z = quantile(residuals_calib, 1 - alpha)  # e.g. 0.8 quantile of absolute residuals
pred = model.predict(features)
interval_lower = pred - z
interval_upper = pred + z
Update residuals_calib after seeing true outcome:
new_resid = abs(true_outcome - pred)
residuals_calib.append(new_resid)
if len(residuals_calib) > N: residuals_calib.popleft()
Pitfall: If sample size for residuals is small initially, quantile estimates are noisy. One might use a slightly larger quantile early on (to be safe). Also, if distribution has heavy tail, the single quantile might not ensure conditional coverage – one could use Adaptive conformal (smaller window yields conditional coverage at cost of some miscoverage if distribution shifts quickly). We also assume a common interval for simplicity; in multivariate (total & margin together), one might need a more complex approach, but we can apply it separately to each target for now.
Pseudo-code for evaluating betting strategy:
bankroll = 0
bets = []
for game in test_games:
    model_total = model.predict_total(game.features)
    edge_over = model_total - game.odd_total_line  # positive means model likes over
    if edge_over > 2:
        outcome = 1 if game.final_total > game.odd_total_line else 0  # 1 if over wins
        profit = +0.909 if outcome==1 else -1  # assuming -110 odds, win yields 0.909 units profit (1/1.1)
        bankroll += profit
        bets.append(('over', profit))
    elif edge_over < -2:
        outcome = 1 if game.final_total < game.odd_total_line else 0
        profit = +0.909 if outcome==1 else -1
        bankroll += profit
        bets.append(('under', profit))
    # similar for spread:
    edge_spread = model_margin - game.odd_point_spread
    ...
# After loop:
n_bets = len(bets)
roi = bankroll / n_bets if n_bets>0 else 0
win_rate = len([b for b in bets if b[1] > 0]) / n_bets
Pitfall: Using closing lines in hindsight is fine for evaluation, but in reality, you’d have to bet at those lines (assuming availability). Also, line shopping (getting -110 odds) is assumed. If odds are different (some overs might be -115), that should be accounted for in profit calc. Another issue: by betting multiple markets on same game (total and spread), you double count that game risk – maybe fine, but risk correlation exists. Our strategy separated triggers by category, which might rarely overlap. Keep track of units staked and units returned precisely. For drawdown, we might simulate timeline:
cum_profit = 0
peak = 0
max_drawdown = 0
for bet in bets_in_chronological_order:
    cum_profit += bet.profit
    if cum_profit > peak: 
        peak = cum_profit
    drawdown = peak - cum_profit
    if drawdown > max_drawdown:
        max_drawdown = drawdown
Then max_drawdown indicates worst case. Pitfall: small sample – one season might have only, say, 100 bets. The results can vary by luck. We might want to simulate multiple seasons (if available) or at least multiple random starting bankroll positions to see distribution. The bootstrap of bet outcomes can serve that.
Parallel ensemble prediction:
If using bootstrap ensemble for intervals:
models = [train_model_bootstrap(data) for _ in range(B)]
preds = [model.predict(game.features) for model in models]
preds.sort()
lower = np.percentile(preds, 10)  # 10th percentile for 80% PI
upper = np.percentile(preds, 90)
Pitfall: Bootstrapping time-series can break temporal order – one might need circular block bootstrap for training sets, etc. But if we assume i.i.d. games for bootstrap training, that’s an approximation. Also, ensure diversity in ensemble; might use different seeds or subsample fractions.
Memory and performance considerations: Storing residuals for conformal or doing bootstrap ensembles is fine here (few thousand games). But if scaling, one must watch memory. Also, repeated retraining (like every day if drift triggers often) could be automated offline during off hours. Logging is crucial: log all model versions, their training periods, and performance, to analyze drift over time.
Team and season encoding pitfalls: If we include team IDs as features (for hierarchical effects), need to avoid dummy trap or high cardinality issues. Possibly encode as one-hot for a small league or use impact encoding (average outcome for that team). Pitfall: new team (expansion) or team changes (Seattle -> OKC type relocations) – historically minor for NBA but something to handle gracefully (maybe treat by franchise continuity).
Lack of significance pitfalls: If an experiment yields no clear improvement, resist the temptation to deploy it just because it “makes sense.” The protocol we set (significance + effect size) is there to guard against confirmation bias. For instance, adding a feature that doesn’t statistically help – we should drop it to keep model simpler. There’s always risk of multiple hypothesis testing though – we try many improvements, some may appear to help by chance. Our rigorous split and test should mitigate that.
Communication of results: In code and reports, clearly tag data sources and versions (e.g. “v3 model on 2023 data vs v4 with new features”). Use visualizations: like a time series plot of cumulative betting profit to show how profit progressed (ensures transparency on when strategy was winning or losing). Or a plot of actual vs predicted over time to show drift.
By following these implementation notes and being cautious of pitfalls, we aim to successfully deploy the recommended changes, with the codebase organized for clarity and the model’s integrity upheld through strict temporal separation of data. Each experiment and change will be carefully validated in code, and results will be reproducible (fix random seeds where needed, maintain version control on data preprocessing). The end result should be a more robust, well-understood NBA prediction system ready for real-world use.
[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [43] [44] [45] [46] [47] [48] [59] [64] [65] [66] [67] [68] [72] [73] statistical_approach.json
file://file_00000000a598722fb3efdd6060e9676b
[41] Diebold-Mariano Test
https://palatej.github.io/pages/stats/tests/forecasts/dm.html
[42] [51] [52] [53] [54] [55] [56] Conformal Prediction Algorithms for Time Series Forecasting: Methods and Benchmark
https://arxiv.org/html/2601.18509v1
[49] Block bootstrapping with time series and spatial data | by JT | Medium
https://medium.com/@jcatankard_76170/block-bootstrapping-with-time-series-and-spatial-data-bd7d7830681e
[50] [61] [62] How to measure conformal prediction performance? — MAPIE 1.2.0 documentation
https://mapie.readthedocs.io/en/latest/theoretical_description_metrics.html
[57] [58] [60] NGBoost: Natural Gradient Boosting for Probabilistic Prediction
https://stanfordmlgroup.github.io/projects/ngboost/
[63] Evaluating Probabilistic Predictions: Proper Scoring Rules
https://huiwenn.github.io/predictive-distributions
[69] [70] A comprehensive guide to population stability index (psi) with a Python example
https://www.nannyml.com/blog/population-stability-index-psi
[71] 8 Concept Drift Detection Methods - AI Infrastructure Alliance
https://ai-infrastructure.org/8-concept-drift-detection-methods/